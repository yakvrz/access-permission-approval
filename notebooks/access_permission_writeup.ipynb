{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Access Permission Approval Prediction\n",
        "\n",
        "Predicting whether internal app permission requests will be approved, using historical decisions plus user, application, and organizational context. Trained with CatBoost and validated with user-grouped cross-validation to avoid leakage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "- Data: 116k historical decisions (3.7k users, 500 apps), balanced target (~50%).\n",
        "- Model: CatBoost classifier on mixed tabular features (app identity/category, org context, machine flag, manager signals).\n",
        "- Validation: GroupKFold by user to mirror deployment on new users; probabilities used for thresholded decisions.\n",
        "- OOF metrics: ROC-AUC **0.932**, PR-AUC **0.930**, Accuracy **0.857**.\n",
        "- Operating point: threshold **0.542** \u2192 TPR ~0.854, FPR ~0.140.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Overview\n",
        "The task is to predict whether an application permission request will be granted or denied, based on historical decisions and associated metadata about users, applications, and organizational context. The goal is to provide accurate, calibrated predictions that could support or automate parts of the decision process. Success is measured by the model's ability to distinguish approvals from denials, maintain consistent performance under leakage-aware validation, and produce probabilities that remain reliable when converted to binary decisions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Audit\n",
        "The dataset comprises 116,063 historical permission decisions involving 3,724 users and 509 applications. Initial exploration revealed:\n",
        "\n",
        "- **Balanced target**: ~50.1% approval rate provides a well-balanced classification problem\n",
        "- **Clean data**: Keys in metadata are unique; there are no contradictory labels for a (user, app) pair and no exact duplicates.\n",
        "- **Missing values**: Significant gaps in organizational fields (40% for department, 22% for office location), while app category shows minimal missingness (3.5%).\n",
        "\n",
        "The high rate of missing organizational data presented an opportunity for feature engineering through manager metadata recovery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis\n",
        "Several patterns emerged from examining main effects:\n",
        "\n",
        "* **Organizational structure**: Departments and offices show substantial, high-volume variation around the global base rate (e.g., department 3 is much higher than average, while departments 1 and 4 are lower).\n",
        "* **User type**: Machine accounts have very low grant rates (~8.5%) and represent ~2% of requests. Informative but low volume.\n",
        "* **Seniority**: Mid-level roles trend higher, but effects are modest compared with app and organizational signals.\n",
        "\n",
        "These findings justify investing in app identity and organizational context while treating rare categories and small groups cautiously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "### Missing Value Handling\n",
        "* Use a sentinel (`__UNKNOWN__`) for missing categoricals so the model can learn missingness as signal.\n",
        "* Join manager metadata (department, office, seniority) and **backfill** user fields only when the manager value is known; otherwise retain the sentinel. This preserves missingness while recovering context.\n",
        "\n",
        "### Identity Management\n",
        "* Exclude `userId` to avoid user-specific memorization and steer the model toward app/org patterns that generalize.\n",
        "* Retain `managerId` to capture team-level norms available at inference time.\n",
        "* Add `manager_seniority_diff` (user seniority - manager seniority) to encode hierarchy; leave NA where no manager exists.\n",
        "\n",
        "### Categorical Encoding Notes\n",
        "* Treat IDs/codes (`appId`, `managerId`, `department_filled`, `office_filled`, `category`) as nominal strings and pass as categorical features to CatBoost.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling and Validation\n",
        "\n",
        "### Model Selection\n",
        "**CatBoost** was chosen for:\n",
        "- Native handling of high-cardinality categorical features\n",
        "- Robust treatment of missing values\n",
        "- Strong performance on mixed-type tabular data\n",
        "\n",
        "### Validation Strategy\n",
        "Evaluation uses **GroupKFold cross-validation** grouped by `userId` to ensure a given user appears in only one fold, preventing leakage from repeated user patterns and targeting the intended generalization scenario: new users on seen apps. AUC is consistent across folds, indicating patterns generalize well.\n",
        "\n",
        "### Performance (OOF)\n",
        "- **ROC-AUC**: 0.932\n",
        "- **PR-AUC**: 0.930\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold Selection\n",
        "Several criteria were considered (maximum accuracy, maximum F1, and a 0.50 reference). With classes nearly balanced and no stated asymmetry in error costs, maximizing accuracy is the neutral choice: it minimizes total error at a single threshold rather than overweighting the positive class.\n",
        "\n",
        "**Selected threshold: t = 0.542**\n",
        "- **Accuracy:** 85.7%\n",
        "- **Precision:** 85.9%\n",
        "- **Recall:** 85.4%\n",
        "- **False positive rate:** 14.0%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Segment Coverage\n",
        "- Evaluated top apps, departments, and manager departments (n>=100) at the chosen threshold.\n",
        "- Per-segment metrics (count, approval rate, accuracy, ROC-AUC when defined) are stored in `reports/segment_metrics.json`.\n",
        "- No single segment shows collapse; highest-volume apps and departments retain accuracy near the global operating point.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Figures\n",
        "- ROC & PR: ![ROC & PR](../docs/figures/roc_pr.png)\n",
        "- Score distribution: ![Score distribution](../docs/figures/score_distribution.png)\n",
        "- Confusion matrix: ![Confusion matrix](../docs/figures/confusion_matrix.png)\n",
        "- Calibration: ![Calibration](../docs/figures/calibration.png)\n",
        "- SHAP importance: ![SHAP importance](../docs/figures/shap_importance.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance\n",
        "CatBoost importances (relative influence on predictions; normalized to 100):\n",
        "\n",
        "* **App**: **28.5%**. Individual applications are the largest source of signal.\n",
        "* **Department**: **20.6%**. Organizational unit is strongly predictive.\n",
        "* **Office**: **11.9%**. Location or functional division adds meaningful context.\n",
        "* **Manager**: **11.5%**. Team-level patterns matter.\n",
        "* **Manager office**: **9.0%**. Manager context reinforces organizational effects.\n",
        "* **Category**: **4.1%**. Coarser app grouping contributes but far less than specific apps.\n",
        "\n",
        "Overall, the model is app-driven and heavily informed by organizational context. Core org features account for ~46% of total importance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Considerations and Risks\n",
        "\n",
        "* **Cost asymmetry**: The chosen operating point assumes roughly symmetric costs; adjust the threshold if costs differ.\n",
        "* **Distribution shifts**: Shifts in app/department mix or approval rate can move the optimal threshold.\n",
        "* **Cold start on new entities**: Expect lower accuracy on new apps/managers/departments/offices until history accrues.\n",
        "* **Segment variability**: Aggregate metrics can hide weaker performance in specific segments; monitor segment-level metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repro & Usage\n",
        "- Train/evaluate: `python -m access_perm.train --config config/default.yaml` (artifacts in `models/` and `reports/`).\n",
        "- Regenerate plots: `python -m access_perm.report --config config/default.yaml`.\n",
        "- Score new requests: `python scripts/predict.py --config config/default.yaml --input data/submission.csv --output reports/predictions.csv`.\n",
        "- No code is embedded here; all logic lives in `src/access_perm` and is covered by tests and CLI entrypoints.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}